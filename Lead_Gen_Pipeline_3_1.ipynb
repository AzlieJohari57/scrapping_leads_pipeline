{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e167a867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import time\n",
    "import asyncio\n",
    "import requests\n",
    "import aiohttp\n",
    "import nest_asyncio\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scrapy\n",
    "from scrapy_playwright.page import PageMethod\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from fuzzywuzzy import fuzz, process\n",
    "\n",
    "from google.oauth2.service_account import Credentials\n",
    "from googleapiclient.discovery import build\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec89c9c",
   "metadata": {},
   "source": [
    "### Getting ACRA List for scrapping to get the names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c166a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path = \"./Staging/Bronze/bronze_data_1.parquet\"\n",
    "if os.path.exists(parquet_path):\n",
    "    acra_data_filtered_by_industry = pd.read_parquet(parquet_path, engine=\"fastparquet\")\n",
    "    print(f\"Loaded {len(acra_data_filtered_by_industry)} rows from {parquet_path}\")\n",
    "    print(acra_data_filtered_by_industry.shape)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Parquet file not found at {parquet_path}\")\n",
    "\n",
    "print(acra_data_filtered_by_industry.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2f756c",
   "metadata": {},
   "source": [
    "### Merge Silver_data_2_Phone with acra_data_filtered_by_industry to get Acra registered Name "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d467ac99",
   "metadata": {},
   "source": [
    "### with phone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb257f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path = \"./Staging/Silver/Silver_data_2_Phone.parquet\"\n",
    "if os.path.exists(parquet_path):\n",
    "    Silver_data_2_Phone = pd.read_parquet(parquet_path, engine=\"fastparquet\")\n",
    "    print(f\"Loaded {len(Silver_data_2_Phone)} rows from {parquet_path}\")\n",
    "    print(Silver_data_2_Phone.shape)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Parquet file not found at {parquet_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cdc3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "With_Phone_df = Silver_data_2_Phone.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88a0b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_search_queries(df):\n",
    "    queries = []\n",
    "    for idx, row in df.iterrows():\n",
    "        entity_name = str(row.get('ENTITY_NAME', '')).strip()\n",
    "        address = str(row.get('operational_address', '')).strip()\n",
    "        \n",
    "        if not entity_name or entity_name == 'nan':\n",
    "            continue\n",
    "            \n",
    "        if address and address != 'nan':\n",
    "            search_query = f\"{entity_name} {address} Singapore\"\n",
    "        else:\n",
    "            search_query = f\"{entity_name} Singapore\"\n",
    "            \n",
    "        queries.append({'idx': idx, 'entity_name': entity_name, 'search_query': search_query})\n",
    "    return queries\n",
    "\n",
    "\n",
    "# Merge With_Phone_df with acra_data_filtered_by_industry to get ENTITY_NAME\n",
    "With_Phone_df = With_Phone_df.merge(\n",
    "    acra_data_filtered_by_industry[['UEN', 'ENTITY_NAME']], \n",
    "    on='UEN', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"Rows: {len(With_Phone_df)}\")\n",
    "print(f\"ENTITY_NAME filled: {With_Phone_df['ENTITY_NAME'].notna().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef34b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "With_Phone_df[[\"UEN\", \"ENTITY_NAME\", \"Phones\"]].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609b4275",
   "metadata": {},
   "source": [
    "### Merge with Silver_data_2_No_Phone with acra_data_filtered_by_industry to get Acra registered Name "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d65287",
   "metadata": {},
   "source": [
    "### without phone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb05cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path = \"./Staging/Silver/Silver_data_2_No_Phone.parquet\"\n",
    "if os.path.exists(parquet_path):\n",
    "    Silver_data_2_No_Phone = pd.read_parquet(parquet_path, engine=\"fastparquet\")\n",
    "    print(f\"Loaded {len(Silver_data_2_No_Phone)} rows from {parquet_path}\")\n",
    "    print(Silver_data_2_No_Phone.shape)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Parquet file not found at {parquet_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff8d5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "No_Phone_df = Silver_data_2_No_Phone.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f127ac00",
   "metadata": {},
   "source": [
    "### Formatting the duplicate phones and source "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8945b900",
   "metadata": {},
   "outputs": [],
   "source": [
    "No_Phone_df['Phones'] = None\n",
    "No_Phone_df['PIC Source 1'] = None\n",
    "\n",
    "No_Phone_df[\"UEN\"].is_unique\n",
    "print(No_Phone_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7f2369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_search_queries(df):\n",
    "    queries = []\n",
    "    for idx, row in df.iterrows():\n",
    "        entity_name = str(row.get('ENTITY_NAME', '')).strip()\n",
    "        address = str(row.get('operational_address', '')).strip()\n",
    "        \n",
    "        if not entity_name or entity_name == 'nan':\n",
    "            continue\n",
    "            \n",
    "        if address and address != 'nan':\n",
    "            search_query = f\"{entity_name} {address} Singapore\"\n",
    "        else:\n",
    "            search_query = f\"{entity_name} Singapore\"\n",
    "            \n",
    "        queries.append({'idx': idx, 'entity_name': entity_name, 'search_query': search_query})\n",
    "    return queries\n",
    "\n",
    "\n",
    "# Merge No_Phone_df with acra_data_filtered_by_industry to get ENTITY_NAME\n",
    "No_Phone_df = No_Phone_df.merge(\n",
    "    acra_data_filtered_by_industry[['UEN', 'ENTITY_NAME']], \n",
    "    on='UEN', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"Rows: {len(No_Phone_df)}\")\n",
    "print(f\"ENTITY_NAME filled: {No_Phone_df['ENTITY_NAME'].notna().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9d0601",
   "metadata": {},
   "outputs": [],
   "source": [
    "No_Phone_df[[\"UEN\", \"ENTITY_NAME\", \"Phones\"]].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b51b937",
   "metadata": {},
   "outputs": [],
   "source": [
    "No_Phone_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a811e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GOOGLE MAPS PHONE NUMBER SEARCH - COST OPTIMIZED\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from apify_client import ApifyClient\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "client = ApifyClient(os.getenv(\"APIFY_API_KEY\"))\n",
    "\n",
    "BATCH_SIZE = 800\n",
    "FUZZY_MATCH_THRESHOLD = 1\n",
    "companies_to_search = No_Phone_df.copy()\n",
    "\n",
    "\n",
    "def validate_singapore_phone(phone):\n",
    "    if not phone:\n",
    "        return None\n",
    "    cleaned = re.sub(r'[\\s\\-\\(\\)\\.\\|/\\+]', '', str(phone))\n",
    "    if cleaned.startswith('65') and len(cleaned) == 10:\n",
    "        number_part = cleaned[2:]\n",
    "        if re.match(r'^[689]\\d{7}$', number_part):\n",
    "            return f\"+65{number_part}\"\n",
    "    elif re.match(r'^[689]\\d{7}$', cleaned):\n",
    "        return f\"+65{cleaned}\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def create_search_queries(df):\n",
    "    queries = []\n",
    "    for idx, row in df.iterrows():\n",
    "        entity_name = str(row.get('ENTITY_NAME', '')).strip()\n",
    "        address = str(row.get('operational_address', '')).strip()\n",
    "        if not entity_name or entity_name == 'nan':\n",
    "            continue\n",
    "        if address and address != 'nan':\n",
    "            search_query = f\"{entity_name} {address} Singapore\"\n",
    "        else:\n",
    "            search_query = f\"{entity_name} Singapore\"\n",
    "        queries.append({'idx': idx, 'entity_name': entity_name, 'search_query': search_query})\n",
    "    return queries\n",
    "\n",
    "\n",
    "def run_google_places_scraper(client, search_queries_batch):\n",
    "    search_strings = [q['search_query'] for q in search_queries_batch]\n",
    "    run_input = {\n",
    "        \"searchStringsArray\": search_strings,\n",
    "        \"maxCrawledPlacesPerSearch\": 1,\n",
    "        \"language\": \"en\",\n",
    "        \"scrapeContacts\": False,\n",
    "        \"scrapePlaceDetailPage\": False,\n",
    "        \"scrapeTableReservationProvider\": False,\n",
    "        \"scrapeDirectories\": False,\n",
    "        \"includeWebResults\": False,\n",
    "        \"maxReviews\": 0,\n",
    "        \"maxImages\": 0,\n",
    "        \"scrapeReviewsPersonalData\": False,\n",
    "        \"skipClosedPlaces\": False,\n",
    "        \"maxQuestions\": 0,\n",
    "        \"maximumLeadsEnrichmentRecords\": 0,\n",
    "    }\n",
    "    try:\n",
    "        run = client.actor(\"compass/crawler-google-places\").call(run_input=run_input)\n",
    "        if not run or not isinstance(run, dict) or 'id' not in run:\n",
    "            print(f\"ERROR: API returned invalid response: {run}\")\n",
    "            return [], \"API returned invalid response\"\n",
    "        run_client = client.run(run[\"id\"])\n",
    "        run_info = run_client.wait_for_finish()\n",
    "        status = run_info.get('status', 'UNKNOWN')\n",
    "        if status in ['FAILED', 'TIMED-OUT', 'ABORTED']:\n",
    "            print(f\"ERROR: Actor run {status}\")\n",
    "            return [], f\"Actor run {status}\"\n",
    "        if status == \"SUCCEEDED\" and \"defaultDatasetId\" in run:\n",
    "            dataset = client.dataset(run[\"defaultDatasetId\"])\n",
    "            return list(dataset.iterate_items()), None\n",
    "        print(f\"ERROR: Scraping failed with status: {status}\")\n",
    "        return [], f\"Scraping failed: {status}\"\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {type(e).__name__}: {str(e)}\")\n",
    "        return [], f\"Error: {type(e).__name__}: {str(e)}\"\n",
    "\n",
    "\n",
    "def fuzzy_match_company(entity_name, google_results, threshold=FUZZY_MATCH_THRESHOLD):\n",
    "    if not google_results or not entity_name:\n",
    "        return None, 0\n",
    "    entity_name_clean = entity_name.upper().strip()\n",
    "    best_match, best_score = None, 0\n",
    "    for result in google_results:\n",
    "        google_name = result.get('title', '') or result.get('name', '')\n",
    "        if not google_name:\n",
    "            continue\n",
    "        google_name_clean = google_name.upper().strip()\n",
    "        max_score = max(\n",
    "            fuzz.ratio(entity_name_clean, google_name_clean),\n",
    "            fuzz.partial_ratio(entity_name_clean, google_name_clean),\n",
    "            fuzz.token_sort_ratio(entity_name_clean, google_name_clean),\n",
    "            fuzz.token_set_ratio(entity_name_clean, google_name_clean)\n",
    "        )\n",
    "        if max_score > best_score:\n",
    "            best_score = max_score\n",
    "            best_match = result\n",
    "    return (best_match, best_score) if best_score >= threshold else (None, best_score)\n",
    "\n",
    "\n",
    "# ---- Main Execution ----\n",
    "search_queries = create_search_queries(companies_to_search)\n",
    "total_queries = len(search_queries)\n",
    "num_batches = (total_queries + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "print(f\"Total search queries: {total_queries}\")\n",
    "print(f\"Number of batches: {num_batches}\")\n",
    "\n",
    "if total_queries == 0:\n",
    "    print(\"WARNING: No search queries generated. Check that ENTITY_NAME column exists and has values.\")\n",
    "\n",
    "all_results = []\n",
    "phones_found = 0\n",
    "\n",
    "for batch_idx in range(0, total_queries, BATCH_SIZE):\n",
    "    batch_num = (batch_idx // BATCH_SIZE) + 1\n",
    "    batch = search_queries[batch_idx:batch_idx + BATCH_SIZE]\n",
    "    print(f\"\\nProcessing batch {batch_num}/{num_batches} ({len(batch)} queries)...\")\n",
    "    \n",
    "    items, error = run_google_places_scraper(client, batch)\n",
    "    \n",
    "    if error:\n",
    "        print(f\"  Batch {batch_num} error: {error}\")\n",
    "        for query in batch:\n",
    "            all_results.append({'idx': query['idx'], 'GMaps_Phone': None, 'GMaps_Status': 'error'})\n",
    "        continue\n",
    "    \n",
    "    print(f\"  Got {len(items)} results from Google Maps\")\n",
    "    \n",
    "    results_by_query = {}\n",
    "    for item in items:\n",
    "        search_string = item.get('searchString', '')\n",
    "        if search_string not in results_by_query:\n",
    "            results_by_query[search_string] = []\n",
    "        results_by_query[search_string].append(item)\n",
    "    \n",
    "    batch_phones = 0\n",
    "    for query in batch:\n",
    "        google_results = results_by_query.get(query['search_query'], [])\n",
    "        if not google_results:\n",
    "            all_results.append({'idx': query['idx'], 'GMaps_Phone': None, 'GMaps_Status': 'no_results'})\n",
    "            continue\n",
    "        best_match, score = fuzzy_match_company(query['entity_name'], google_results)\n",
    "        if best_match:\n",
    "            raw_phone = best_match.get('phone') or best_match.get('phoneUnformatted')\n",
    "            validated_phone = validate_singapore_phone(raw_phone) if raw_phone else None\n",
    "            if validated_phone:\n",
    "                phones_found += 1\n",
    "                batch_phones += 1\n",
    "            all_results.append({'idx': query['idx'], 'GMaps_Phone': validated_phone, 'GMaps_Status': 'matched'})\n",
    "        else:\n",
    "            all_results.append({'idx': query['idx'], 'GMaps_Phone': None, 'GMaps_Status': 'no_match'})\n",
    "    \n",
    "    print(f\"  Batch {batch_num}: {batch_phones} phones found\")\n",
    "    \n",
    "    if batch_idx + BATCH_SIZE < total_queries:\n",
    "        time.sleep(2)\n",
    "\n",
    "# Update No_Phone_df\n",
    "for result in all_results:\n",
    "    if result['GMaps_Phone'] and result['GMaps_Status'] == 'matched':\n",
    "        idx = result['idx']\n",
    "        if idx in No_Phone_df.index:\n",
    "            No_Phone_df.loc[idx, 'Phones'] = result['GMaps_Phone']\n",
    "            No_Phone_df.loc[idx, 'PIC Source 1'] = \"Google\"\n",
    "\n",
    "# Split into two DataFrames\n",
    "Google_Mapped_Scrapped_with_Phone = No_Phone_df[\n",
    "    No_Phone_df['Phones'].notna() & \n",
    "    (No_Phone_df['Phones'] != '')\n",
    "].copy()\n",
    "\n",
    "Google_Mapped_Scrapped_No_Phone = No_Phone_df[\n",
    "    No_Phone_df['Phones'].isna() | \n",
    "    (No_Phone_df['Phones'] == '')\n",
    "].copy()\n",
    "\n",
    "# Count unique phones\n",
    "unique_phones = Google_Mapped_Scrapped_with_Phone['Phones'].nunique()\n",
    "\n",
    "# Final Summary\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"SUCCESS\")\n",
    "print(f\"Unique phones found: {unique_phones}\")\n",
    "print(f\"No phone: {len(Google_Mapped_Scrapped_No_Phone)}\")\n",
    "print(f\"Google_Mapped_Scrapped_with_Phone: {len(Google_Mapped_Scrapped_with_Phone)} rows\")\n",
    "print(f\"Google_Mapped_Scrapped_No_Phone: {len(Google_Mapped_Scrapped_No_Phone)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2642d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "Google_Mapped_Scrapped_with_Phone\n",
    "print(Google_Mapped_Scrapped_with_Phone.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf67d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Google_Mapped_Scrapped_with_Phone = Google_Mapped_Scrapped_with_Phone[[\"UEN\", \"ENTITY_NAME\", \"Phones\", \"PIC Source 1\", \"Emails\", \"Website\", \"Facebook\", \"LinkedIn\", \"Instagram\", \"TikTok\", \"operational_street\", \"operational_unit\", \"operational_postal_code\",  \"operational_address\", ]]\n",
    "Google_Mapped_Scrapped_with_Phone.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f7e6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(With_Phone_df.columns.tolist())\n",
    "With_Phone_df = With_Phone_df[[\"UEN\", \"ENTITY_NAME\", \"Phones\", \"PIC Source 1\", \"Emails\", \"Website\", \"Facebook\", \"LinkedIn\", \"Instagram\", \"TikTok\", \"operational_street\", \"operational_unit\", \"operational_postal_code\",  \"operational_address\", ]]\n",
    "With_Phone_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08be36b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "phones_unique_1 = With_Phone_df[\"Phones\"].apply(lambda x: tuple(x) if isinstance(x, list) else x).is_unique\n",
    "phones_unique_2 = Google_Mapped_Scrapped_with_Phone[\"Phones\"].apply(lambda x: tuple(x) if isinstance(x, list) else x).is_unique\n",
    "uen_unique_1 = With_Phone_df[\"UEN\"].is_unique\n",
    "uen_unique_2 = Google_Mapped_Scrapped_with_Phone[\"UEN\"].is_unique\n",
    "\n",
    "print(f\"With_Phone_df Phones unique: {phones_unique_1}\")\n",
    "print(f\"Google_Mapped Phones unique: {phones_unique_2}\")\n",
    "print(f\"With_Phone_df UEN unique: {uen_unique_1}\")\n",
    "print(f\"Google_Mapped UEN unique: {uen_unique_2}\")\n",
    "\n",
    "if all([phones_unique_1, phones_unique_2, uen_unique_1, uen_unique_2]):\n",
    "    With_Phone_df = pd.concat([With_Phone_df, Google_Mapped_Scrapped_with_Phone], ignore_index=True)\n",
    "    print(f\"Appended. With_Phone_df now has {len(With_Phone_df)} rows\")\n",
    "else:\n",
    "    print(\"NOT appended â€” duplicate values detected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563774f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "With_Phone_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33121ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "With_Phone_df.to_parquet(\"./Staging/Gold/Gold_Scrapped_Data_1.parquet\", index=False, engine=\"fastparquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
